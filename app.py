from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import json
import numpy as np
import faiss
import google.generativeai as genai

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FLASK SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = Flask(__name__)

# âœ… Povolit volÃ¡nÃ­ jen z tvÃ© Netlify strÃ¡nky
CORS(
    app,
    resources={r"/ask": {"origins": ["https://cosmic-crostata-1c51df.netlify.app"]}},
    methods=["GET", "POST", "OPTIONS"],
    allow_headers=["Content-Type"]
)

# (NepovinnÃ¡ zÃ¡loha CORS hlaviÄek)
@app.after_request
def add_cors_headers(response):
    response.headers["Access-Control-Allow-Origin"] = "https://cosmic-crostata-1c51df.netlify.app"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "Content-Type"
    return response

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PROMÄšNNÃ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
index = None
chunks = None
chat_histories = {}

# ğŸ” Gemini API klÃ­Ä
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
gemini_model = genai.GenerativeModel("gemini-1.5-flash-latest")

# ğŸ” Funkce pro embedding dotazu pÅ™es Gemini
def get_embedding(text):
    response = genai.embed_content(
        model="models/embedding-001",
        content=text,
        task_type="retrieval_query"
    )
    return np.array([response["embedding"]], dtype="float32")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API ENDPOINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/ask", methods=["POST"])
def ask():
    global index, chunks, chat_histories

    data = request.get_json()
    question = data.get("question", "")
    profile = data.get("profileName", "unknown").lower()

    # ğŸ§  NaÄÃ­st index a texty, pokud jeÅ¡tÄ› nejsou naÄtenÃ©
    if index is None:
        index = faiss.read_index("faiss.index")
        with open("chunks.json", "r") as f:
            chunks = json.load(f)

    # ğŸ” Vyhledat relevantnÃ­ ÄÃ¡sti
    query_embedding = get_embedding(question)
    D, I = index.search(query_embedding, k=5)
    relevant_chunks = [chunks[i] for i in I[0]]
    context = "\n".join(relevant_chunks)

    # ğŸ’¬ SprÃ¡va historie dotazÅ¯
    chat_histories.setdefault(profile, []).append(f"UÅ¾ivatel: {question}")
    chat_histories[profile] = chat_histories[profile][-3:]
    history_prompt = "\n".join(chat_histories[profile])

    # ğŸ§  VytvoÅ™enÃ­ promÄ›nnÃ© pro celÃ½ prompt
    system_prompt = f"""Jsi El_KapitÃ¡n â€“ bÃ½valÃ½ zÃ¡vodnÃ­k a teÄ drsnej trenÃ©r bÄ›Å¾eckÃ©ho lyÅ¾ovÃ¡nÃ­. TrÃ©nujeÅ¡ juniory z Prahy, kteÅ™Ã­ to myslÃ­ vÃ¡Å¾nÄ›, ale nÄ›kdy potÅ™ebujÃ­ nakopnout. MluvÃ­Å¡ jako kÃ¡moÅ¡, co Å™Ã­kÃ¡ vÄ›ci na rovinu â€“ obÄas drsnÄ›, obÄas vtipnÄ›, ale vÅ¾dycky napÅ™Ã­mo. Trochu sarkasmus, Å¾Ã¡dnÃ½ kecy.

OdpovÃ­dÃ¡Å¡ struÄnÄ›, jasnÄ› a PÅ˜ÃMO na otÃ¡zku. NeÅ™eÅ¡, co by â€zÃ¡leÅ¾eloâ€œ â€“ rozhodni. KdyÅ¾ se tÄ› nÄ›kdo ptÃ¡, co mÃ¡ dÄ›lat, tak mu to Å™ekni rovnou, jako kdybys stÃ¡l vedle nÄ›j u trati.

TrÃ©ninky piÅ¡ konkrÃ©tnÄ›. PÅ™Ã­klad:  
â€BÄ›h 75 min v tempu, poslednÃ­ch 15 min I3. Pak 5Ã—100 m sprinty do kopce. A bulkovat.â€œ

Neomlouvej se, nepiÅ¡ Å¾Ã¡dnÃ© obecnÃ© Å™eÄi, neodkazuj na trenÃ©ry ani zdroje.  
TvÅ¯j styl je drsnÃ½, efektivnÃ­ a kÃ¡moÅ¡skej.  
KdyÅ¾ je nÄ›co blbÄ›, klidnÄ› to Å™ekni.  
KdyÅ¾ je nÄ›kdo lÃ­nej, poÅ¡li ho na kolce nebo do posilky.  
NejseÅ¡ chatbot. JseÅ¡ KapitÃ¡n.
OdpovÃ­dÃ¡Å¡ pÅ™Ã­mo na poslednÃ­ otÃ¡zku.
Zde je kontext pro inspiraci:
{context}

PoslednÃ­ zprÃ¡vy:
{history_prompt}
"""

    try:
        response = gemini_model.generate_content(system_prompt)
        return jsonify({"answer": response.text})
    except Exception as e:
        return jsonify({"answer": f"Chyba: {e}"})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RUN PRO RENDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port)
