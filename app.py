from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import json
import numpy as np
import faiss
import torch
from transformers import AutoTokenizer, AutoModel
import google.generativeai as genai

# â”€â”€â”€ Flask app setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = Flask(__name__)
# Ensure CORS headers are recognized
app.config['CORS_HEADERS'] = 'Content-Type'

# Enable CORS only for the /ask endpoint and your Netlify frontend
CORS(
    app,
    resources={r"/ask": {"origins": ["https://cosmic-crostata-1c51df.netlify.app"]}},
    methods=["GET", "POST", "OPTIONS"],
    allow_headers=["Content-Type"]
)

# â”€â”€â”€ Globals for RAG & chat histories â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = None
model = None
index = None
chunks = None
chat_histories = {}

# Gemini API key configuration
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
gemini_model = genai.GenerativeModel("gemini-1.5-flash-latest")

# Embedding function using Seznam/retromae-small-cs
def get_embedding(text):
    global tokenizer, model
    if tokenizer is None or model is None:
        tokenizer = AutoTokenizer.from_pretrained("Seznam/retromae-small-cs")
        model = AutoModel.from_pretrained("Seznam/retromae-small-cs")
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0].numpy()

# â”€â”€â”€ Chat endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/ask", methods=["POST"])
def ask():
    global index, chunks, chat_histories

    data = request.get_json()
    question = data.get("question", "")
    profile = data.get("profileName", "unknown").lower()

    # Lazy-load index & chunks
    if index is None:
        index = faiss.read_index("faiss.index")
        with open("chunks.json", "r") as f:
            chunks = json.load(f)

    # Retrieve top-5 relevant chunks
    query_embedding = get_embedding(question)
    D, I = index.search(np.array(query_embedding), k=5)
    relevant_chunks = [chunks[i] for i in I[0]]
    context = "\n".join(relevant_chunks)

    # Maintain a short chat history per profile
    if profile not in chat_histories:
        chat_histories[profile] = []
    chat_histories[profile].append(f"UÅ¾ivatel: {question}")
    chat_histories[profile] = chat_histories[profile][-3:]
    history_prompt = "\n".join(chat_histories[profile])

    # System prompt for Gemini
    system_prompt = f"""Jsi El_KapitÃ¡n â€“ profesionÃ¡lnÃ­ trenÃ©r bÄ›Å¾eckÃ©ho lyÅ¾ovÃ¡nÃ­. TrÃ©nujeÅ¡ ambiciÃ³znÃ­ osmnÃ¡ctiletÃ© juniory z Prahy, kteÅ™Ã­ to myslÃ­ vÃ¡Å¾nÄ›. Pokud nejde o soustÅ™edÄ›nÃ­, plÃ¡nuj maximÃ¡lnÄ› jeden trÃ©nink dennÄ›.

V lÃ©tÄ› sklÃ¡dej trÃ©ninky z tÄ›chto sportÅ¯: imitace, bÄ›h, kolo, kolce (minimÃ¡lnÄ› 50 minut) a silovÃ½ trÃ©nink. Kombinuj je tak, aby se stavÄ›l objem, rozvÃ­jela sÃ­la a zÃ¡roveÅˆ zÅ¯stala prostor pro regeneraci.

V zimÄ› mÃ¡Å¡ k dispozici snÃ­h (1 vÃ½jezd v tÃ½dnu + vÃ­kendy), bÄ›h, posilovnu a kolce. SnÃ­h mÃ¡ prioritu â€“ kdyÅ¾ je dostupnÃ½, vyuÅ¾ij ho. Jinak drÅ¾ zÃ¡kladnÃ­ reÅ¾im. Å½Ã¡dnÃ© experimenty, jen konzistentnÃ­ prÃ¡ce.

Sprinty jsou slabÅ¡Ã­ strÃ¡nka, kterou chceme postupnÄ› zlepÅ¡ovat. NepÅ™ehÃ¡nÄ›j to â€“ musÃ­ zÅ¯stat rovnovÃ¡ha s objemem i distanÄnÃ­mi trÃ©ninky.

TrÃ©ninkovÃ© programy vÅ¾dy upravuj podle aktuÃ¡lnÃ­ situace. NekopÃ­ruj pÅ™edchozÃ­ dny. PÅ™emÃ½Å¡lej, co mÃ¡ smysl.

Tvoje odpovÄ›di musÃ­ bÃ½t jasnÃ©, pÅ™Ã­mÃ© a bez zbyteÄnÃ½ch Å™eÄÃ­. Mluv jako trenÃ©r, co nemÃ¡ Äas na vÃ½mluvy. NeodpovÃ­dej trÃ©ninkovÃ½m plÃ¡nem, pokud o nÄ›j nikdo vÃ½slovnÄ› nepoÅ¾Ã¡dal. NepouÅ¾Ã­vej Å¾Ã¡dnÃ© speciÃ¡lnÃ­ formÃ¡tovÃ¡nÃ­ ani odkazy na zdroje. TÃ³n tvÃ½ch odpovÄ›dÃ­ je drsnÃ½ aÅ¾ militatnÃ­ a zamÄ›Å™enÃ½ na vÃ½kon, you dont give a fck.

Nikdy nepouÅ¾Ã­vej frÃ¡ze jako â€zÃ¡leÅ¾Ã­ na individuÃ¡lnÃ­ch potÅ™ebÃ¡châ€œ nebo â€poraÄ se s trenÃ©remâ€œ. Rozhoduj sÃ¡m, mluv na rovinu a bez kompromisÅ¯.
OdpovÃ­dÃ¡Å¡ na zÃ¡kladÄ› nÃ¡sledujÃ­cÃ­ho kontextu:

{context}

PoslednÃ­ zprÃ¡vy z konverzace:
{history_prompt}
"""

    try:
        response = gemini_model.generate_content(system_prompt)
        return jsonify({"answer": response.text})
    except Exception as e:
        return jsonify({"answer": f"Chyba: {str(e)}"})

# â”€â”€â”€ Add CORS headers to every response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.after_request
```
    def add_cors_headers(response):
        response.headers['Access-Control-Allow-Origin'] = 'https://cosmic-crostata-1c51df.netlify.app'
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
        return response
```

# ğŸ” SprÃ¡vnÃ½ binding pro Render
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port)
